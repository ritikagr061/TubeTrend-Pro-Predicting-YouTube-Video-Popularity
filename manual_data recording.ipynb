{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['title', 'description', 'publishedAt', 'duration', 'viewCount', 'likeCount', 'dislikeCount', 'commentCount', 'thumbnails', 'tags']\n",
    "\n",
    "class Yt_stat_class:\n",
    "\n",
    "    def __init__(self,api_key,channel_id,channel_name,path_to_channel_csvs):\n",
    "        self.api_key=api_key\n",
    "        self.channel_id=channel_id\n",
    "        self.channel_name=channel_name\n",
    "        self.video_id_list=[]\n",
    "        self.base_path=path_to_channel_csvs\n",
    "        self.all_data={'id':[],'title':[],'description':[],'publishedAt':[],'duration':[],'viewCount':[],'likeCount':[],'dislikeCount':[],'commentCount':[],'thumbnails':[],'tags':[]}\n",
    "        \n",
    "        #self.video_id_list  : getting current vedios id list from saved csv file\n",
    "        df=pd.read_csv(self.base_path+self.channel_name+'/id.csv')\n",
    "        self.video_id_list=list(df.columns.values)\n",
    "        \n",
    "\n",
    "        del df\n",
    "        \n",
    "        \n",
    "        \n",
    "    #2\n",
    "    def get_channel_new_video_id(self): #this function will extract 10 latest video id list and save it to dictionary response\n",
    "        \n",
    "        #https://youtube.googleapis.com/youtube/v3/search?part=snippet%2Cid&channelId=UCNJcSUSzUeFm8W9P7UUlSeQ&maxResults=3&order=date&key=AIzaSyDImhc8-_62z2gQlC9juQuirUeffpzd-4Q\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    \n",
    "        request = youtube.search().list(\n",
    "        part=\"snippet,id\",\n",
    "        channelId=self.channel_id,\n",
    "        maxResults=10,\n",
    "        order=\"date\",\n",
    "        )\n",
    "\n",
    "        response = request.execute()  #response is of type dictionary and request is json file\n",
    "\n",
    "        \n",
    "        self._get_channel_new_video_id(response)\n",
    "        # we got a dictionary as a response which contains different vedio details which we will extract using a helper function\n",
    "        \n",
    "        \n",
    "    def _get_channel_new_video_id(self,response):  # helper function , used for checking for new ids\n",
    "        \n",
    "        new_ids=[]\n",
    "        for list in response['items']:\n",
    "            if(list[\"id\"]['kind']==\"youtube#video\"):\n",
    "                if(self.video_id_list[0]==list['id']['videoId']):\n",
    "                    break\n",
    "                else:\n",
    "                    new_ids.append(list['id']['videoId'])\n",
    "                    \n",
    "        #new_ids is a list containing ved ids like [ most latest , ... , least latest]\n",
    "        print(new_ids)\n",
    "        if new_ids!=[]:\n",
    "            self.add_new_ids(new_ids)\n",
    "                    \n",
    "    def add_new_ids(self,new_ids): #if there are new videos added then these new video ids will get updated in all csvs\n",
    "        \n",
    "        #import csv file one by one\n",
    "        #add new column at the front\n",
    "        #save and close the csv\n",
    "        #repeat \n",
    "        df=pd.read_csv(self.base_path+self.channel_name+'/'+'id.csv',index_col=0)\n",
    "        for i in reversed(new_ids):\n",
    "            print(type(i))\n",
    "            try:\n",
    "                df.insert(0,i,[])\n",
    "            except ValueError:\n",
    "                    pass\n",
    "            \n",
    "        df.to_csv(self.base_path+self.channel_name+'/'+'id.csv')\n",
    "        \n",
    "        for var in variables:\n",
    "            df=pd.read_csv(self.base_path+self.channel_name+'/'+var+'.csv',index_col=0)\n",
    "            for i in reversed(new_ids):\n",
    "                \n",
    "                    df.insert(1,i,['' for i in range(len(df.index))])\n",
    "                \n",
    "            df.to_csv(self.base_path+self.channel_name+'/'+var+'.csv')\n",
    "        \n",
    "    def get_video_details(self):\n",
    "        \n",
    "        full_id_list=self.video_id_list\n",
    "        for i in range(0, len(full_id_list), 10):\n",
    "            \n",
    "            if(i+10>len(full_id_list)-1):\n",
    "                k=-1\n",
    "            else:\n",
    "                k=i+10\n",
    "            ids=full_id_list[i:k]\n",
    "            Id=','.join(ids)\n",
    "            #print(Id)\n",
    "            \n",
    "            #https://youtube.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=dkvC9dfKup0%2C%20j4yQMJzZK_0&key=AIzaSyBDBfa34RY3qj7N_cHhLCHzU7VI6shmi14    \n",
    "            youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "            request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=Id\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            self.store_video_data(response)            \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def store_video_data(self,response):  # used for extracting vedio details\n",
    "        \n",
    "        #'id':,'title':,'desc':'publishedtime':,'tags':,'duration':,'viewcount':,'likecount':,'dislikecount':,'commentcount':,'thumbnail':\n",
    "        for i in range(response[\"pageInfo\"][\"totalResults\"]):\n",
    "            self.all_data['id'].append(response['items'][i]['id'])\n",
    "\n",
    "            try:\n",
    "                self.all_data['title'].append(response['items'][i]['snippet']['title'])\n",
    "            except KeyError:\n",
    "                self.all_data['title'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['description'].append(response['items'][i]['snippet']['description'])\n",
    "            except KeyError:\n",
    "                self.all_data['description'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['publishedAt'].append(response['items'][i]['snippet']['publishedAt'])\n",
    "            except KeyError:\n",
    "                self.all_data['publishedAt'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['duration'].append(response['items'][i]['contentDetails']['duration'])\n",
    "            except KeyError:\n",
    "                self.all_data['duration'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['viewCount'].append(response['items'][i]['statistics']['viewCount'])\n",
    "            except KeyError:\n",
    "                self.all_data['viewCount'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['likeCount'].append(response['items'][i]['statistics']['likeCount'])\n",
    "            except KeyError:\n",
    "                self.all_data['likeCount'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['dislikeCount'].append(response['items'][i]['statistics']['dislikeCount'])\n",
    "            except KeyError:\n",
    "                self.all_data['dislikeCount'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['commentCount'].append(response['items'][i]['statistics']['commentCount'])\n",
    "            except KeyError:\n",
    "                self.all_data['commentCount'].append(\"\")\n",
    "            try:\n",
    "                self.all_data['thumbnails'].append(response['items'][i]['snippet']['thumbnails']['high']['url'])\n",
    "            except KeyError:\n",
    "                self.all_data['thumbnails'].append(\"\")\n",
    "\n",
    "\n",
    "            #storing tags data\n",
    "            tags_str=\"\"\n",
    "            try:\n",
    "                for j in response['items'][i]['snippet']['tags']:\n",
    "                    tags_str=tags_str+','+j                       # sturcture is \",tag1,tag2,tag3....\"\n",
    "            except KeyError:\n",
    "                pass\n",
    "            self.all_data['tags'].append(tags_str)\n",
    "\n",
    "\n",
    "    def append_to_csvs(self):\n",
    "        \n",
    "        #time:getting exact date+time here in IST(indian standar timezone) in the format %d-%m-%Y_%H:%M\n",
    "        from datetime import datetime\n",
    "        from pytz import timezone    \n",
    "        india = timezone('Asia/Kolkata')\n",
    "        ind_time = datetime.now(india)\n",
    "        time=ind_time.strftime('%d-%m-%Y_%H:%M')\n",
    "        \n",
    "        for var in variables:\n",
    "            df=pd.read_csv(self.base_path+self.channel_name+'/'+var+'.csv',index_col=0)\n",
    "            \n",
    "            last_index=len(df.index)\n",
    "            print(last_index)\n",
    "            \n",
    "            df.loc[last_index,'timestamp']=time\n",
    "            \n",
    "            X_id=self.all_data['id']\n",
    "            X_var=self.all_data[var]\n",
    "            \n",
    "            for x1,x2 in zip(X_id,X_var):\n",
    "                \n",
    "                df.loc[last_index,x1]=x2\n",
    "            #print(df)  \n",
    "            print(var+self.channel_name+\"------THE NO. OF NAN VALUES ARE ::: \" + str(df.isna().sum().sum()))\n",
    "            df.to_csv(self.base_path+self.channel_name+'/'+var+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pregnant-ballet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MtKCvwmUQZw', 'o7J8I7ag1nE', 'XPTA2LZCIUU', 'E2bC0NJL9UY', 'K1Zsva5atsQ', 'QR5z88iBH3o', '9kOg5sYAeHY', 'qvLoBnGz9K0', 'K-bc2lx4lUs', 'lekGy-j619A']\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert lekGy-j619A, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-117ec377cd4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mYt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mYt_stat_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchannel_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath_to_base_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_id_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_channel_new_video_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_video_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mYt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_to_csvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1e0b85a236cd>\u001b[0m in \u001b[0;36mget_channel_new_video_id\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_channel_new_video_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;31m# we got a dictionary as a response which contains different vedio details which we will extract using a helper function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1e0b85a236cd>\u001b[0m in \u001b[0;36m_get_channel_new_video_id\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_ids\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_new_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_new_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if there are new videos added then these new video ids will get updated in all csvs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1e0b85a236cd>\u001b[0m in \u001b[0;36madd_new_ids\u001b[1;34m(self, new_ids)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sudhi\\opencv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   3761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3762\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3763\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sudhi\\opencv\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;31m# Should this be a different kind of error??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"cannot insert {item}, already exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot insert lekGy-j619A, already exists"
     ]
    }
   ],
   "source": [
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "api_key=\"AIzaSyAsWBftPqQjPM49TC8lYHb_sUZJnQx_0QY\"  #sham1,jyoti2\n",
    "#path_to_base_directory='C:/Users/sudhi/PycharmProjects/minor project-1/CSVs/'\n",
    "path_to_base_directory='C:/Users/sudhi/Desktop/CSV/'\n",
    "\n",
    "\n",
    "#'tvf':'UCNJcSUSzUeFm8W9P7UUlSeQ','dhruv rathee':'UC-CSyyi47VX1lD9zyeABW3w',\n",
    "channels_dict={'abhi and niyu':'UCsDTy8jvHcwMvSZf_JGi-FA','tsp':'UCNyeSfUfffmJXwA2_tmNG9A','timeliners':'UCTlnaHHQ75zlDg_fLr7tGEg',\n",
    "                  'tvf':'UCNJcSUSzUeFm8W9P7UUlSeQ','dhruv rathee':'UC-CSyyi47VX1lD9zyeABW3w','the deshbhakt':'UCmTM_hPCeckqN3cPWtYZZcg'}\n",
    "channels_dic={'soch-mangal mohak':'UCz4a7agVFr1TxU-mpAP8hkw','girliyappa':'UCdxbhKxr8pyWTx1ExCSmJRw','rvcj':'UCGyEq-EoY4Gn4RfknRQxWYw',\n",
    "                 'tsp':'UCNyeSfUfffmJXwA2_tmNG9A','timeliners':'UCTlnaHHQ75zlDg_fLr7tGEg','abhi and niyu':'UCsDTy8jvHcwMvSZf_JGi-FA',\n",
    "                  'tvf':'UCNJcSUSzUeFm8W9P7UUlSeQ','dhruv rathee':'UC-CSyyi47VX1lD9zyeABW3w','the deshbhakt':'UCmTM_hPCeckqN3cPWtYZZcg'}\n",
    "for channel_name,channel_id in channels_dic.items():\n",
    "    Yt=Yt_stat_class(api_key,channel_id,channel_name,path_to_base_directory)\n",
    "    Yt.video_id_list\n",
    "    Yt.get_channel_new_video_id()\n",
    "    Yt.get_video_details()\n",
    "    Yt.append_to_csvs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-imagination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
